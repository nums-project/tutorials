{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Import and Initialize Modin and NumS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-02 13:02:27,041\tINFO services.py:1265 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using driver node ip as head node.\n",
      "head node 10.142.52.89\n",
      "total cpus 4.0\n",
      "device_grid (0, 0) 0=node:10.142.52.89/cpu:1\n"
     ]
    }
   ],
   "source": [
    "import modin.pandas as pd\n",
    "\n",
    "import nums\n",
    "from nums.experimental.nums_modin import from_modin\n",
    "import nums.numpy as nps\n",
    "nums.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Load and Preprocess Dataset with Modin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 737 ms, sys: 69.3 ms, total: 806 ms\n",
      "Wall time: 1.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "higgs_train = pd.read_csv(\"training.zip\")\n",
    "higgs_train.loc[higgs_train['Label'] == 'b', 'Label'] = 0\n",
    "higgs_train.loc[higgs_train['Label'] == 's', 'Label'] = 1\n",
    "higgs_train = higgs_train.drop(columns=['EventId'])\n",
    "columns = higgs_train.columns.values\n",
    "X_columns, y_column = columns[:-1], columns[-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Convert Modin DataFrame to NumS BlockArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = from_modin(higgs_train[X_columns].astype(float))\n",
    "weights = X_train[:, -1]\n",
    "X_train = X_train[:, :-1]\n",
    "# Drop weight column from names.\n",
    "X_columns = X_columns[:-1]\n",
    "y_train = from_modin(higgs_train[y_column].astype(int)).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_missing = False\n",
    "impute_missing = False\n",
    "keep_columns = []\n",
    "kept_column_names = []\n",
    "for j in range(X_train.shape[1]):\n",
    "    mask = X_train[:, j] == -999.0\n",
    "    num_nans = nps.sum(mask)\n",
    "    frac = num_nans / X_train.shape[0]\n",
    "    if drop_missing and frac > 0.7:\n",
    "        print(\"drop\", j, X_columns[j])\n",
    "        continue\n",
    "    keep_columns.append(j)\n",
    "    kept_column_names.append(X_columns[j])\n",
    "    if impute_missing and frac > 0.0:\n",
    "        X_train[mask, j] = nps.nanmean(X_train[:, j])\n",
    "        print(\"impute\", j, X_columns[j])\n",
    "X_train = X_train[:, keep_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.1 ms, sys: 11.3 ms, total: 52.4 ms\n",
      "Wall time: 237 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Center X and compute covariance.\n",
    "n, d = X_train.shape\n",
    "X_centered = X_train - nps.mean(X_train, axis=0)\n",
    "assert nps.allclose(nps.mean(X_centered, axis=0), nps.zeros((d,)))\n",
    "\n",
    "# Compute Covariance Matrix.\n",
    "C = (X_centered.T @ X_centered) / (n-1)\n",
    "\n",
    "# Compute PCA via SVD.\n",
    "V, S, VT = nps.linalg.svd(C)\n",
    "assert nps.allclose(V, VT.T)\n",
    "pc = X_train @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.9410306560525489\n",
      "1 0.9940381637372118\n",
      "2 0.9996230624635641\n",
      "3 0.9999181743384001\n",
      "4 0.9999972582402463\n",
      "5 0.9999984533073456\n",
      "6 0.9999992569556514\n",
      "7 0.9999996406346516\n",
      "8 0.9999998537795541\n",
      "9 0.9999999182955481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RuntimeWarning: Operation cumsum not implemented, falling back to NumPy. If this is too slow or failing, please open an issue on GitHub.\n"
     ]
    }
   ],
   "source": [
    "eigen_vals = S**2 / (n - 1)\n",
    "explained_variance = eigen_vals / nps.sum(eigen_vals)\n",
    "for i, val in enumerate(nps.cumsum(explained_variance).get()[:10]):\n",
    "    print(i, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRI_jet_leading_pt\n",
      "PRI_jet_leading_eta\n",
      "PRI_jet_leading_phi\n",
      "DER_mass_jet_jet\n",
      "PRI_jet_subleading_pt\n",
      "DER_deltaeta_jet_jet\n",
      "DER_lep_eta_centrality\n",
      "PRI_jet_subleading_phi\n",
      "PRI_jet_subleading_eta\n",
      "DER_prodeta_jet_jet\n",
      "DER_mass_MMC\n",
      "PRI_met_sumet\n",
      "DER_sum_pt\n",
      "PRI_jet_all_pt\n",
      "DER_pt_h\n",
      "DER_mass_transverse_met_lep\n",
      "PRI_met\n",
      "PRI_tau_pt\n",
      "PRI_lep_pt\n",
      "DER_pt_tot\n",
      "DER_mass_vis\n",
      "PRI_jet_num\n",
      "DER_met_phi_centrality\n",
      "DER_deltar_tau_lep\n",
      "DER_pt_ratio_lep_tau\n",
      "PRI_lep_eta\n",
      "PRI_tau_eta\n",
      "PRI_met_phi\n",
      "PRI_tau_phi\n",
      "PRI_lep_phi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RuntimeWarning: Operation argsort not implemented, falling back to NumPy. If this is too slow or failing, please open an issue on GitHub.\n"
     ]
    }
   ],
   "source": [
    "components = VT\n",
    "sorted_importance = nps.argsort(-nps.sum(nps.abs(components[:2]), axis=0)).get()\n",
    "for col in X_columns[sorted_importance]:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from nums.sklearn import (train_test_split, \n",
    "                          StandardScaler, \n",
    "                          GaussianNB, \n",
    "                          LogisticRegression, \n",
    "                          SVC, \n",
    "                          MLPClassifier, \n",
    "                          GradientBoostingClassifier, \n",
    "                          RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Final metric.\n",
    "def metric(ytrue, ypred, weights):\n",
    "    # Use accuracy for now.\n",
    "    return 1.0 - nps.sum(nps.abs(ytrue - ypred))/ytrue.shape[0]\n",
    "\n",
    "    # TODO: Auto-convert numeric operands in element-wise ops.\n",
    "    import numpy as np\n",
    "    ytrue, ypred, weights = ytrue.get(), ypred.get(), weights.get()\n",
    "    \"\"\" Approximate Median Significance defined as:\n",
    "        AMS = sqrt(\n",
    "                2 { (s + b + b_r) log[1 + (s/(b+b_r))] - s}\n",
    "              )\n",
    "    where b_r = 10, b = background, s = signal, log is natural logarithm \"\"\"\n",
    "    # Max AMS on training set is 67.7\n",
    "\n",
    "    # True-positive rate.\n",
    "    s = np.sum(weights[(ytrue == 1) & (ytrue == ypred)])\n",
    "    # False-positive rate.\n",
    "    b = np.sum(weights[(ytrue == 1) & (ytrue != ypred)])\n",
    "    br = 10.0\n",
    "    radicand = 2 * ((s + b + br) * np.log(1.0 + s / (b + br)) - s)\n",
    "    return np.sqrt(radicand)\n",
    "assert nps.allclose(nps.array(1.0), metric(y_train, y_train, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-02 13:08:17,102\tWARNING worker.py:1215 -- The actor or task with ID ffffffffffffffffed9d662ab305d6ac4e1b07b001000000 cannot be scheduled right now. You can ignore this message if this Ray cluster is expected to auto-scale or if you specified a runtime_env for this actor or task, which may take time to install.  Otherwise, this is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increasing the resources available to this Ray cluster.\n",
      "Required resources for this actor or task: {CPU: 1.000000}, {node:10.142.52.89: 0.000100}\n",
      "Available resources on this node: {0.000000/4.000000 CPU, 30.851660 GiB/30.851660 GiB memory, 1.000000/1.000000 GPU, 15.425830 GiB/15.425830 GiB object_store_memory, 0.999600/1.000000 node:10.142.52.89, 1.000000/1.000000 accelerator_type:GTX}\n",
      "In total there are 1 pending tasks and 1 pending actors on this node.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 8 pipelines.\n",
      "CPU times: user 437 ms, sys: 107 ms, total: 544 ms\n",
      "Wall time: 17.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "scores = []\n",
    "for drop_features in [0, 3]:\n",
    "    if drop_features > 0:\n",
    "        feature_mask = nps.zeros(shape=sorted_importance.shape, dtype=bool)\n",
    "        feature_mask[sorted_importance[:-drop_features]] = True\n",
    "        Xt, Xv, yt, yv, wt, wv = train_test_split(X_train[:, feature_mask], y_train, weights)\n",
    "    else:\n",
    "        Xt, Xv, yt, yv, wt, wv = train_test_split(X_train, y_train, weights)\n",
    "    numfeatstr = \"num_feats=%s\" % Xt.shape[1]\n",
    "    for p_cls in [StandardScaler, None]:\n",
    "        if p_cls is None:\n",
    "            ppstr = \"preproc=None\"\n",
    "            pXt = Xt\n",
    "            pXv = Xv\n",
    "        else:\n",
    "            ppstr = \"preproc=\" + p_cls.__name__\n",
    "            p_inst = p_cls()\n",
    "            pXt = p_inst.fit_transform(Xt)\n",
    "            pXv = p_inst.fit_transform(Xv)\n",
    "\n",
    "        # Tree-based Ensemble Methods\n",
    "        for n_estimators in [30]:\n",
    "            for max_depth in [4]:\n",
    "                for max_features in [None]:\n",
    "                    m = RandomForestClassifier(n_estimators=n_estimators, \n",
    "                                               max_depth=max_depth, max_features=max_features)\n",
    "                    m.fit(pXt, yt)\n",
    "                    scores.append([(numfeatstr + \", \"\n",
    "                                    + ppstr + \", \"\n",
    "                                    + m.__class__.__name__\n",
    "                                    + (\"(%s, %s, %s)\" % (n_estimators, max_depth, max_features))),\n",
    "                                   (m.predict(pXv), yv, wv)])\n",
    "                    for learning_rate in [.4]:\n",
    "                        for subsample in [.9]:\n",
    "                            m = GradientBoostingClassifier(n_estimators=n_estimators, \n",
    "                                                           max_depth=max_depth, \n",
    "                                                           max_features=max_features,\n",
    "                                                           learning_rate=learning_rate,\n",
    "                                                           subsample=subsample)\n",
    "                            m.fit(pXt, yt)\n",
    "                            scores.append([(numfeatstr + \", \"\n",
    "                                            + ppstr + \", \"\n",
    "                                            + m.__class__.__name__ \n",
    "                                            + (\"(%s, %s, %s, %s, %s)\" % (n_estimators, \n",
    "                                                                max_depth, \n",
    "                                                                max_features,\n",
    "                                                                learning_rate,\n",
    "                                                                subsample))),\n",
    "                                           (m.predict(pXv), yv, wv)])\n",
    "print(\"Training %s pipelines.\" % len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 88.1 ms, sys: 21.8 ms, total: 110 ms\n",
      "Wall time: 950 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for res in scores:\n",
    "    res[-1] = metric(*res[-1]).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_feats=30, preproc=None, NumsGradientBoostingClassifier(20, 2, None, 0.4, 0.9) 0.8203199999999999\n",
      "num_feats=27, preproc=None, NumsGradientBoostingClassifier(20, 2, None, 0.4, 0.9) 0.819296\n",
      "num_feats=30, preproc=NumsStandardScaler, NumsGradientBoostingClassifier(20, 2, None, 0.4, 0.9) 0.8042400000000001\n",
      "num_feats=27, preproc=NumsStandardScaler, NumsGradientBoostingClassifier(20, 2, None, 0.4, 0.9) 0.8017920000000001\n",
      "num_feats=30, preproc=NumsStandardScaler, NumsRandomForestClassifier(20, 2, None) 0.7744\n",
      "num_feats=30, preproc=None, NumsRandomForestClassifier(20, 2, None) 0.7712479999999999\n",
      "num_feats=27, preproc=None, NumsRandomForestClassifier(20, 2, None) 0.768752\n",
      "num_feats=27, preproc=NumsStandardScaler, NumsRandomForestClassifier(20, 2, None) 0.767968\n"
     ]
    }
   ],
   "source": [
    "for res in sorted(scores, key=lambda x: -x[-1]):\n",
    "    print(*res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "- https://www.kaggle.com/c/higgs-boson/code\n",
    "- https://nycdatascience.com/blog/student-works/top2p-higgs-boson-machine-learning/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
