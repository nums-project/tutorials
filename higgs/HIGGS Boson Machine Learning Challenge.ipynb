{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Import and Initialize Modin and NumS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-04 00:06:52,186\tINFO services.py:1265 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using driver node ip as head node.\n",
      "head node 192.168.0.168\n",
      "total cpus 4.0\n",
      "device_grid (0, 0) 0=node:192.168.0.168/cpu:1\n"
     ]
    }
   ],
   "source": [
    "import modin.pandas as pd\n",
    "import nums\n",
    "import nums.numpy as nps\n",
    "nums.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Load and Preprocess Dataset with Modin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.51 s, sys: 176 ms, total: 1.68 s\n",
      "Wall time: 2.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "higgs_train = pd.read_csv(\"training.zip\")\n",
    "higgs_train.loc[higgs_train['Label'] == 'b', 'Label'] = 0\n",
    "higgs_train.loc[higgs_train['Label'] == 's', 'Label'] = 1\n",
    "higgs_train = higgs_train.drop(columns=['EventId'])\n",
    "columns = higgs_train.columns.values\n",
    "X_columns, y_column = columns[:-1], columns[-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Convert Modin DataFrame to NumS BlockArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = nums.from_modin(higgs_train[X_columns].astype(float))\n",
    "weights = X_train[:, -1]\n",
    "X_train = X_train[:, :-1]\n",
    "# Drop weight column from names.\n",
    "X_columns = X_columns[:-1]\n",
    "y_train = nums.from_modin(higgs_train[y_column].astype(int)).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_missing = False\n",
    "impute_missing = False\n",
    "keep_columns = []\n",
    "kept_column_names = []\n",
    "for j in range(X_train.shape[1]):\n",
    "    mask = X_train[:, j] == -999.0\n",
    "    num_nans = nps.sum(mask)\n",
    "    frac = num_nans / X_train.shape[0]\n",
    "    if drop_missing and frac > 0.7:\n",
    "        print(\"drop\", j, X_columns[j])\n",
    "        continue\n",
    "    keep_columns.append(j)\n",
    "    kept_column_names.append(X_columns[j])\n",
    "    if impute_missing and frac > 0.0:\n",
    "        X_train[mask, j] = nps.nanmean(X_train[:, j])\n",
    "        print(\"impute\", j, X_columns[j])\n",
    "X_train = X_train[:, keep_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 66 ms, sys: 5.43 ms, total: 71.4 ms\n",
      "Wall time: 131 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Compute PCA via SVD.\n",
    "C = nps.cov(X_train, rowvar=False)\n",
    "V, S, VT = nps.linalg.svd(C)\n",
    "assert nps.allclose(V, VT.T)\n",
    "pc = X_train @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.9410306560525489\n",
      "1 0.9940381637372118\n",
      "2 0.9996230624635641\n",
      "3 0.9999181743384001\n",
      "4 0.9999972582402463\n",
      "5 0.9999984533073456\n",
      "6 0.9999992569556514\n",
      "7 0.9999996406346516\n",
      "8 0.9999998537795541\n",
      "9 0.9999999182955481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RuntimeWarning: Operation cumsum not implemented, falling back to NumPy. If this is too slow or failing, please open an issue on GitHub.\n"
     ]
    }
   ],
   "source": [
    "eigen_vals = S**2 / (X_train.shape[0] - 1)\n",
    "explained_variance = eigen_vals / nps.sum(eigen_vals)\n",
    "for i, val in enumerate(nps.cumsum(explained_variance).get()[:10]):\n",
    "    print(i, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRI_jet_leading_pt\n",
      "PRI_jet_leading_eta\n",
      "PRI_jet_leading_phi\n",
      "DER_mass_jet_jet\n",
      "PRI_jet_subleading_pt\n",
      "DER_deltaeta_jet_jet\n",
      "DER_lep_eta_centrality\n",
      "PRI_jet_subleading_phi\n",
      "PRI_jet_subleading_eta\n",
      "DER_prodeta_jet_jet\n",
      "DER_mass_MMC\n",
      "PRI_met_sumet\n",
      "DER_sum_pt\n",
      "PRI_jet_all_pt\n",
      "DER_pt_h\n",
      "DER_mass_transverse_met_lep\n",
      "PRI_met\n",
      "PRI_tau_pt\n",
      "PRI_lep_pt\n",
      "DER_pt_tot\n",
      "DER_mass_vis\n",
      "PRI_jet_num\n",
      "DER_met_phi_centrality\n",
      "DER_deltar_tau_lep\n",
      "DER_pt_ratio_lep_tau\n",
      "PRI_lep_eta\n",
      "PRI_tau_eta\n",
      "PRI_met_phi\n",
      "PRI_tau_phi\n",
      "PRI_lep_phi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RuntimeWarning: Operation argsort not implemented, falling back to NumPy. If this is too slow or failing, please open an issue on GitHub.\n"
     ]
    }
   ],
   "source": [
    "components = VT\n",
    "sorted_importance = nps.argsort(-nps.sum(nps.abs(components[:2]), axis=0)).get()\n",
    "for col in X_columns[sorted_importance]:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from nums.sklearn import (train_test_split, \n",
    "                          StandardScaler, \n",
    "                          GaussianNB, \n",
    "                          LogisticRegression, \n",
    "                          SVC, \n",
    "                          MLPClassifier, \n",
    "                          GradientBoostingClassifier, \n",
    "                          RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.71112289505676\n"
     ]
    }
   ],
   "source": [
    "# Final metric.\n",
    "def metric(ytrue, ypred, weights):\n",
    "    \"\"\" Approximate Median Significance defined as:\n",
    "        AMS = sqrt(\n",
    "                2 { (s + b + b_r) log[1 + (s/(b+b_r))] - s}\n",
    "              )\n",
    "    where b_r = 10, b = background, s = signal, log is natural logarithm \"\"\"\n",
    "    # Max AMS on training set is 67.7\n",
    "    # return 1.0 - nps.sum(nps.abs(ytrue - ypred))/ytrue.shape[0]\n",
    "    # True-positive rate.\n",
    "    s = nps.sum(weights[(ytrue == 1) & (ytrue == ypred)])\n",
    "    # False-positive rate.\n",
    "    b = nps.sum(weights[(ytrue == 1) & (ytrue != ypred)])\n",
    "    br = 10.0\n",
    "    radicand = 2 * ((s + b + br) * nps.log(1.0 + s / (b + br)) - s)\n",
    "    return nps.sqrt(radicand)\n",
    "\n",
    "print(metric(y_train, y_train, weights).get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drop_features 0\n",
      "drop_features 3\n",
      "Training 8 pipelines.\n",
      "CPU times: user 880 ms, sys: 159 ms, total: 1.04 s\n",
      "Wall time: 19.7 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-04 00:14:22,263\tWARNING worker.py:1215 -- The actor or task with ID ffffffffffffffff3587967316529495f212dc1301000000 cannot be scheduled right now. You can ignore this message if this Ray cluster is expected to auto-scale or if you specified a runtime_env for this actor or task, which may take time to install.  Otherwise, this is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increasing the resources available to this Ray cluster.\n",
      "Required resources for this actor or task: {node:192.168.0.168: 0.000100}, {CPU: 1.000000}\n",
      "Available resources on this node: {0.000000/4.000000 CPU, 29.992548 GiB/29.992548 GiB memory, 1.000000/1.000000 GPU, 14.996274 GiB/14.996274 GiB object_store_memory, 1.000000/1.000000 accelerator_type:GTX, 0.999600/1.000000 node:192.168.0.168}\n",
      "In total there are 0 pending tasks and 3 pending actors on this node.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "scores = []\n",
    "for drop_features in [0, 3]:\n",
    "    if drop_features > 0:\n",
    "        feature_mask = nps.zeros(shape=sorted_importance.shape, dtype=bool)\n",
    "        feature_mask[sorted_importance[:-drop_features]] = True\n",
    "        Xt, Xv, yt, yv, wt, wv = train_test_split(X_train[:, feature_mask], y_train, weights)\n",
    "    else:\n",
    "        Xt, Xv, yt, yv, wt, wv = train_test_split(X_train, y_train, weights)\n",
    "    print(\"drop_features\", drop_features)\n",
    "    numfeatstr = \"num_feats=%s\" % Xt.shape[1]\n",
    "    for p_cls in [StandardScaler, None]:\n",
    "        if p_cls is None:\n",
    "            ppstr = \"preproc=None\"\n",
    "            pXt = Xt\n",
    "            pXv = Xv\n",
    "        else:\n",
    "            ppstr = \"preproc=\" + p_cls.__name__\n",
    "            p_inst = p_cls()\n",
    "            pXt = p_inst.fit_transform(Xt)\n",
    "            pXv = p_inst.fit_transform(Xv)\n",
    "\n",
    "        # Tree-based Ensemble Methods\n",
    "        for n_estimators in [10]:\n",
    "            for max_depth in [2]:\n",
    "                for max_features in [None]:\n",
    "                    m = RandomForestClassifier(n_estimators=n_estimators, \n",
    "                                               max_depth=max_depth, max_features=max_features)\n",
    "                    m.fit(pXt, yt)\n",
    "                    scores.append([(numfeatstr + \", \"\n",
    "                                    + ppstr + \", \"\n",
    "                                    + m.__class__.__name__\n",
    "                                    + (\"(%s, %s, %s)\" % (n_estimators, max_depth, max_features))),\n",
    "                                   (m.predict(pXv), yv, wv)])\n",
    "                    for learning_rate in [.4]:\n",
    "                        for subsample in [.9]:\n",
    "                            m = GradientBoostingClassifier(n_estimators=n_estimators, \n",
    "                                                           max_depth=max_depth, \n",
    "                                                           max_features=max_features,\n",
    "                                                           learning_rate=learning_rate,\n",
    "                                                           subsample=subsample)\n",
    "                            m.fit(pXt, yt)\n",
    "                            scores.append([(numfeatstr + \", \"\n",
    "                                            + ppstr + \", \"\n",
    "                                            + m.__class__.__name__ \n",
    "                                            + (\"(%s, %s, %s, %s, %s)\" % (n_estimators, \n",
    "                                                                max_depth, \n",
    "                                                                max_features,\n",
    "                                                                learning_rate,\n",
    "                                                                subsample))),\n",
    "                                           (m.predict(pXv), yv, wv)])\n",
    "print(\"Training %s pipelines.\" % len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.63 s, sys: 323 ms, total: 1.95 s\n",
      "Wall time: 26.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for res in scores:\n",
    "    res[-1] = metric(*res[-1]).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_feats=27, preproc=None, NumsGradientBoostingClassifier(10, 2, None, 0.4, 0.9) 1.269384930400543\n",
      "num_feats=30, preproc=None, NumsGradientBoostingClassifier(10, 2, None, 0.4, 0.9) 1.258370485103042\n",
      "num_feats=27, preproc=NumsStandardScaler, NumsGradientBoostingClassifier(10, 2, None, 0.4, 0.9) 1.2471542638097393\n",
      "num_feats=30, preproc=NumsStandardScaler, NumsGradientBoostingClassifier(10, 2, None, 0.4, 0.9) 1.2183461009279437\n",
      "num_feats=30, preproc=None, NumsRandomForestClassifier(10, 2, None) 0.9869143385415494\n",
      "num_feats=27, preproc=None, NumsRandomForestClassifier(10, 2, None) 0.9841719268750823\n",
      "num_feats=27, preproc=NumsStandardScaler, NumsRandomForestClassifier(10, 2, None) 0.9841086576817014\n",
      "num_feats=30, preproc=NumsStandardScaler, NumsRandomForestClassifier(10, 2, None) 0.9746846523215774\n"
     ]
    }
   ],
   "source": [
    "for res in sorted(scores, key=lambda x: -x[-1]):\n",
    "    print(*res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "- https://www.kaggle.com/c/higgs-boson/code\n",
    "- https://nycdatascience.com/blog/student-works/top2p-higgs-boson-machine-learning/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
