{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically increasing RLIMIT_NOFILE to max value of 1048576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: Not all Ray Dashboard dependencies were found. To use the dashboard please install Ray using `pip install ray[default]`. To disable this message, set RAY_DISABLE_IMPORT_WARNING env var to '1'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using driver node ip as head node.\n",
      "head node 172.31.4.230\n",
      "total cpus 46.0\n",
      "device_grid (0, 0) 0=node:172.31.4.230/cpu:1\n"
     ]
    }
   ],
   "source": [
    "import modin.pandas as pd\n",
    "import nums\n",
    "import nums.numpy as nps\n",
    "nums.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Load and preprocess dataset with Modin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.25 s, sys: 182 ms, total: 1.43 s\n",
      "Wall time: 1.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "higgs_train = pd.read_csv(\"training.zip\")\n",
    "higgs_train.loc[higgs_train['Label'] == 'b', 'Label'] = 0\n",
    "higgs_train.loc[higgs_train['Label'] == 's', 'Label'] = 1\n",
    "higgs_train = higgs_train.drop(columns=['EventId'])\n",
    "columns = higgs_train.columns.values\n",
    "X_columns, y_column = columns[:-1], columns[-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Convert Modin DataFrame to NumS BlockArray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 593 ms, sys: 154 ms, total: 747 ms\n",
      "Wall time: 557 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = nums.from_modin(higgs_train[X_columns].astype(float))\n",
    "weights = X_train[:, -1]\n",
    "X_train = X_train[:, :-1]\n",
    "# Drop weight column from names.\n",
    "X_columns = X_columns[:-1]\n",
    "y_train = nums.from_modin(higgs_train[y_column].astype(int)).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute principal components of dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 250 ms, sys: 99 ms, total: 349 ms\n",
      "Wall time: 1.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Compute PCA via SVD.\n",
    "C = nps.cov(X_train, rowvar=False)\n",
    "V, S, VT = nps.linalg.svd(C)\n",
    "assert nps.allclose(V, VT.T)\n",
    "pc = X_train @ V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute eigen values from singular values, and explained variance from eigen values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.9410306560525487\n",
      "1 0.9940381637372117\n",
      "2 0.999623062463564\n",
      "3 0.9999181743384\n",
      "4 0.9999972582402462\n",
      "5 0.9999984533073455\n",
      "6 0.9999992569556513\n",
      "7 0.9999996406346515\n",
      "8 0.999999853779554\n",
      "9 0.999999918295548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RuntimeWarning: Operation cumsum not implemented, falling back to NumPy. If this is too slow or failing, please open an issue on GitHub.\n"
     ]
    }
   ],
   "source": [
    "eigen_vals = S**2 / (X_train.shape[0] - 1)\n",
    "explained_variance = eigen_vals / nps.sum(eigen_vals)\n",
    "for i, val in enumerate(nps.cumsum(explained_variance).get()[:10]):\n",
    "    print(i, val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order features by explained variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRI_jet_leading_pt\n",
      "PRI_jet_leading_eta\n",
      "PRI_jet_leading_phi\n",
      "DER_mass_jet_jet\n",
      "PRI_jet_subleading_pt\n",
      "DER_deltaeta_jet_jet\n",
      "DER_lep_eta_centrality\n",
      "PRI_jet_subleading_phi\n",
      "PRI_jet_subleading_eta\n",
      "DER_prodeta_jet_jet\n",
      "DER_mass_MMC\n",
      "PRI_met_sumet\n",
      "DER_sum_pt\n",
      "PRI_jet_all_pt\n",
      "DER_pt_h\n",
      "DER_mass_transverse_met_lep\n",
      "PRI_met\n",
      "PRI_tau_pt\n",
      "PRI_lep_pt\n",
      "DER_pt_tot\n",
      "DER_mass_vis\n",
      "PRI_jet_num\n",
      "DER_met_phi_centrality\n",
      "DER_deltar_tau_lep\n",
      "DER_pt_ratio_lep_tau\n",
      "PRI_lep_eta\n",
      "PRI_tau_eta\n",
      "PRI_met_phi\n",
      "PRI_tau_phi\n",
      "PRI_lep_phi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RuntimeWarning: Operation argsort not implemented, falling back to NumPy. If this is too slow or failing, please open an issue on GitHub.\n"
     ]
    }
   ],
   "source": [
    "components = VT\n",
    "sorted_variance = nps.argsort(-nps.sum(nps.abs(components[:2]), axis=0)).get()\n",
    "for col in X_columns[sorted_variance]:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import scikit-learn models from nums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models imported.\n"
     ]
    }
   ],
   "source": [
    "from nums.sklearn import (train_test_split, \n",
    "                          StandardScaler, \n",
    "                          GaussianNB, \n",
    "                          LogisticRegression, \n",
    "                          SVC, \n",
    "                          MLPClassifier, \n",
    "                          GradientBoostingClassifier, \n",
    "                          RandomForestClassifier)\n",
    "print(\"Models imported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the performance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric defined.\n"
     ]
    }
   ],
   "source": [
    "def metric(ytrue, ypred, weights):\n",
    "    \"\"\" Approximate Median Significance defined as:\n",
    "        AMS = sqrt(\n",
    "                2 { (s + b + b_r) log[1 + (s/(b+b_r))] - s}\n",
    "              )\n",
    "    where b_r = 10, b = background, s = signal, log is natural logarithm \"\"\"\n",
    "    # True-positive rate.\n",
    "    s = nps.sum(weights[(ytrue == 1) & (ytrue == ypred)])\n",
    "    # False-positive rate.\n",
    "    b = nps.sum(weights[(ytrue == 1) & (ytrue != ypred)])\n",
    "    br = 10.0\n",
    "    radicand = 2 * ((s + b + br) * nps.log(1.0 + s / (b + br)) - s)\n",
    "    return nps.sqrt(radicand)\n",
    "print(\"Metric defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conduct a search over a small set of feature sets, preprocessors, and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drop_features 0\n",
      "drop_features 3\n",
      "Training 8 pipelines.\n",
      "CPU times: user 128 ms, sys: 19.7 ms, total: 148 ms\n",
      "Wall time: 885 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "scores = []\n",
    "for drop_features in [0, 3]:\n",
    "    if drop_features > 0:\n",
    "        feature_mask = nps.zeros(shape=sorted_variance.shape, dtype=bool)\n",
    "        feature_mask[sorted_variance[:-drop_features]] = True\n",
    "        Xt, Xv, yt, yv, wt, wv = train_test_split(X_train[:, feature_mask], y_train, weights)\n",
    "    else:\n",
    "        Xt, Xv, yt, yv, wt, wv = train_test_split(X_train, y_train, weights)\n",
    "    numfeatstr = \"num_feats=%s\" % Xt.shape[1]\n",
    "    for p_cls in [StandardScaler, None]:\n",
    "        if p_cls is None:\n",
    "            ppstr = \"preproc=None\"\n",
    "            pXt = Xt\n",
    "            pXv = Xv\n",
    "        else:\n",
    "            ppstr = \"preproc=\" + p_cls.__name__\n",
    "            p_inst = p_cls()\n",
    "            pXt = p_inst.fit_transform(Xt)\n",
    "            pXv = p_inst.fit_transform(Xv)\n",
    "\n",
    "        # Tree-based Ensemble Methods\n",
    "        for n_estimators in [10]:\n",
    "            for max_depth in [2]:\n",
    "                for max_features in [None]:\n",
    "                    m = RandomForestClassifier(n_estimators=n_estimators, \n",
    "                                               max_depth=max_depth, max_features=max_features)\n",
    "                    m.fit(pXt, yt)\n",
    "                    scores.append([(numfeatstr + \", \"\n",
    "                                    + ppstr + \", \"\n",
    "                                    + m.__class__.__name__\n",
    "                                    + (\"(%s, %s, %s)\" % (n_estimators, max_depth, max_features))),\n",
    "                                   (m.predict(pXv), yv, wv)])\n",
    "                    for learning_rate in [.4]:\n",
    "                        for subsample in [.9]:\n",
    "                            m = GradientBoostingClassifier(n_estimators=n_estimators, \n",
    "                                                           max_depth=max_depth, \n",
    "                                                           max_features=max_features,\n",
    "                                                           learning_rate=learning_rate,\n",
    "                                                           subsample=subsample)\n",
    "                            m.fit(pXt, yt)\n",
    "                            scores.append([(numfeatstr + \", \"\n",
    "                                            + ppstr + \", \"\n",
    "                                            + m.__class__.__name__ \n",
    "                                            + (\"(%s, %s, %s, %s, %s)\" % (n_estimators, \n",
    "                                                                max_depth, \n",
    "                                                                max_features,\n",
    "                                                                learning_rate,\n",
    "                                                                subsample))),\n",
    "                                           (m.predict(pXv), yv, wv)])\n",
    "print(\"Training %s pipelines.\" % len(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run performance metric and sort the pipelines by their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_feats=27, preproc=None, NumsGradientBoostingClassifier(10, 2, None, 0.4, 0.9) 1.2594896089016074\n",
      "num_feats=30, preproc=None, NumsGradientBoostingClassifier(10, 2, None, 0.4, 0.9) 1.2501577148553882\n",
      "num_feats=27, preproc=NumsStandardScaler, NumsGradientBoostingClassifier(10, 2, None, 0.4, 0.9) 1.2485718059653912\n",
      "num_feats=30, preproc=NumsStandardScaler, NumsGradientBoostingClassifier(10, 2, None, 0.4, 0.9) 1.2297814074454088\n",
      "num_feats=30, preproc=None, NumsRandomForestClassifier(10, 2, None) 0.9982379850930554\n",
      "num_feats=30, preproc=NumsStandardScaler, NumsRandomForestClassifier(10, 2, None) 0.995172325721902\n",
      "num_feats=27, preproc=None, NumsRandomForestClassifier(10, 2, None) 0.9865626382441374\n",
      "num_feats=27, preproc=NumsStandardScaler, NumsRandomForestClassifier(10, 2, None) 0.9789352787033203\n",
      "CPU times: user 776 ms, sys: 162 ms, total: 937 ms\n",
      "Wall time: 9.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = []\n",
    "for res in scores:\n",
    "    results.append((res[0], metric(*res[1]).get()))\n",
    "for res in sorted(results, key=lambda x: -x[-1]):\n",
    "    print(*res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "- https://www.kaggle.com/c/higgs-boson/code\n",
    "- https://nycdatascience.com/blog/student-works/top2p-higgs-boson-machine-learning/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
