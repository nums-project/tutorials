{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import modin.pandas as pd\n",
    "import nums\n",
    "import nums.numpy as nps\n",
    "nums.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Load and preprocess dataset with Modin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "higgs_train = pd.read_csv(\"training.zip\")\n",
    "higgs_train.loc[higgs_train['Label'] == 'b', 'Label'] = 0\n",
    "higgs_train.loc[higgs_train['Label'] == 's', 'Label'] = 1\n",
    "higgs_train = higgs_train.drop(columns=['EventId'])\n",
    "columns = higgs_train.columns.values\n",
    "X_columns, y_column = columns[:-1], columns[-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Convert Modin DataFrame to NumS BlockArray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_train = nums.from_modin(higgs_train[X_columns].astype(float))\n",
    "weights = X_train[:, -1]\n",
    "X_train = X_train[:, :-1]\n",
    "# Drop weight column from names.\n",
    "X_columns = X_columns[:-1]\n",
    "y_train = nums.from_modin(higgs_train[y_column].astype(int)).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute principal components of dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Compute PCA via SVD.\n",
    "C = nps.cov(X_train, rowvar=False)\n",
    "V, S, VT = nps.linalg.svd(C)\n",
    "assert nps.allclose(V, VT.T)\n",
    "pc = X_train @ V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute eigen values from singular values, and explained variance from eigen values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "eigen_vals = S**2 / (X_train.shape[0] - 1)\n",
    "explained_variance = eigen_vals / nps.sum(eigen_vals)\n",
    "for i, val in enumerate(nps.cumsum(explained_variance).get()[:10]):\n",
    "    print(i, val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order features by explained variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "components = VT\n",
    "sorted_variance = nps.argsort(-nps.sum(nps.abs(components[:2]), axis=0)).get()\n",
    "for col in X_columns[sorted_variance]:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import scikit-learn models from nums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from nums.sklearn import (train_test_split, \n",
    "                          StandardScaler, \n",
    "                          GaussianNB, \n",
    "                          LogisticRegression, \n",
    "                          SVC, \n",
    "                          MLPClassifier, \n",
    "                          GradientBoostingClassifier, \n",
    "                          RandomForestClassifier)\n",
    "print(\"Models imported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the performance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def metric(ytrue, ypred, weights):\n",
    "    \"\"\" Approximate Median Significance defined as:\n",
    "        AMS = sqrt(\n",
    "                2 { (s + b + b_r) log[1 + (s/(b+b_r))] - s}\n",
    "              )\n",
    "    where b_r = 10, b = background, s = signal, log is natural logarithm \"\"\"\n",
    "    # True-positive rate.\n",
    "    s = nps.sum(weights[(ytrue == 1) & (ytrue == ypred)])\n",
    "    # False-positive rate.\n",
    "    b = nps.sum(weights[(ytrue == 1) & (ytrue != ypred)])\n",
    "    br = 10.0\n",
    "    radicand = 2 * ((s + b + br) * nps.log(1.0 + s / (b + br)) - s)\n",
    "    return nps.sqrt(radicand)\n",
    "print(\"Metric defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conduct a search over a small set of feature sets, preprocessors, and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "scores = []\n",
    "for drop_features in [0, 3]:\n",
    "    if drop_features > 0:\n",
    "        feature_mask = nps.zeros(shape=sorted_variance.shape, dtype=bool)\n",
    "        feature_mask[sorted_variance[:-drop_features]] = True\n",
    "        Xt, Xv, yt, yv, wt, wv = train_test_split(X_train[:, feature_mask], y_train, weights)\n",
    "    else:\n",
    "        Xt, Xv, yt, yv, wt, wv = train_test_split(X_train, y_train, weights)\n",
    "    print(\"drop_features\", drop_features)\n",
    "    numfeatstr = \"num_feats=%s\" % Xt.shape[1]\n",
    "    for p_cls in [StandardScaler, None]:\n",
    "        if p_cls is None:\n",
    "            ppstr = \"preproc=None\"\n",
    "            pXt = Xt\n",
    "            pXv = Xv\n",
    "        else:\n",
    "            ppstr = \"preproc=\" + p_cls.__name__\n",
    "            p_inst = p_cls()\n",
    "            pXt = p_inst.fit_transform(Xt)\n",
    "            pXv = p_inst.fit_transform(Xv)\n",
    "\n",
    "        # Tree-based Ensemble Methods\n",
    "        for n_estimators in [10]:\n",
    "            for max_depth in [2]:\n",
    "                for max_features in [None]:\n",
    "                    m = RandomForestClassifier(n_estimators=n_estimators, \n",
    "                                               max_depth=max_depth, max_features=max_features)\n",
    "                    m.fit(pXt, yt)\n",
    "                    scores.append([(numfeatstr + \", \"\n",
    "                                    + ppstr + \", \"\n",
    "                                    + m.__class__.__name__\n",
    "                                    + (\"(%s, %s, %s)\" % (n_estimators, max_depth, max_features))),\n",
    "                                   (m.predict(pXv), yv, wv)])\n",
    "                    for learning_rate in [.4]:\n",
    "                        for subsample in [.9]:\n",
    "                            m = GradientBoostingClassifier(n_estimators=n_estimators, \n",
    "                                                           max_depth=max_depth, \n",
    "                                                           max_features=max_features,\n",
    "                                                           learning_rate=learning_rate,\n",
    "                                                           subsample=subsample)\n",
    "                            m.fit(pXt, yt)\n",
    "                            scores.append([(numfeatstr + \", \"\n",
    "                                            + ppstr + \", \"\n",
    "                                            + m.__class__.__name__ \n",
    "                                            + (\"(%s, %s, %s, %s, %s)\" % (n_estimators, \n",
    "                                                                max_depth, \n",
    "                                                                max_features,\n",
    "                                                                learning_rate,\n",
    "                                                                subsample))),\n",
    "                                           (m.predict(pXv), yv, wv)])\n",
    "print(\"Training %s pipelines.\" % len(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run performance metric and sort the pipelines by their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "results = []\n",
    "for res in scores:\n",
    "    results.append((res[0], metric(*res[1]).get()))\n",
    "for res in sorted(results, key=lambda x: -x[-1]):\n",
    "    print(*res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "- https://www.kaggle.com/c/higgs-boson/code\n",
    "- https://nycdatascience.com/blog/student-works/top2p-higgs-boson-machine-learning/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
